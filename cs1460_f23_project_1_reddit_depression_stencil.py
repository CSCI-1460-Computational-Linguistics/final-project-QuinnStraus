# -*- coding: utf-8 -*-
"""CS1460 F23 Project 1: Reddit Depression Stencil

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uj1yfLa3Q7pFGn4RieUvf2hcv_niBs0a

# Reddit Depression Final Project
Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621

Read through the paper fully before starting the assignment!
"""

!pip install happiestfuntokenizing

import numpy as np
import pandas as pd
from sklearn.model_selection import cross_validate, cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier
from google.colab import drive
import pickle

FILEPATH = 'drive/My Drive/'
drive.mount('/content/drive',  force_remount=True)

"""## Preprocessing"""

def load():
  """Load pickles"""
  with open(f"{FILEPATH}Copy of student.pkl", 'rb') as f:
    df = pickle.load(f)
    f.close()
  return df

df = load()

# A map from symptom name to included subreddits
subreddits = {
    "Anger": ["Anger"],
    "Anhedonia": ["anhedonia", "DeadBedrooms",],
    "Anxiety": ["Anxiety", "AnxietyDepression", "HealthAnxiety", "PanicAttack"],
    "Disordered Eating": ["bingeeating", "BingeEatingDisorder", "EatingDisorders", "eating_disorders", "EDAnonymous"],
    "Loneliness": ["ForeverAlone", "lonely"],
    "Sad mood": ["cry", "grief", "sad", "Sadness"],
    "Self-loathing": ["AvPD", "SelfHate", "selfhelp", "socialanxiety", "whatsbotheringyou"],
    "Sleep problem": ["insomnia", "sleep"],
    "Somatic complaint": ["cfs", "ChronicPain", "Constipation", "EssentialTremor", "headaches", "ibs", "tinnitus"],
    "Worthlessness": ["Guilt", "Pessimism", "selfhelp", "whatsbotheringyou"]
}

# List of depression subreddits in the paper
depression_subreddits = ["Anger",
    "anhedonia", "DeadBedrooms",
    "Anxiety", "AnxietyDepression", "HealthAnxiety", "PanicAttack",
    "DecisionMaking", "shouldi",
    "bingeeating", "BingeEatingDisorder", "EatingDisorders", "eating_disorders", "EDAnonymous",
    "chronicfatigue", "Fatigue",
    "ForeverAlone", "lonely",
    "cry", "grief", "sad", "Sadness",
    "AvPD", "SelfHate", "selfhelp", "socialanxiety", "whatsbotheringyou",
    "insomnia", "sleep",
    "cfs", "ChronicPain", "Constipation", "EssentialTremor", "headaches", "ibs", "tinnitus",
    "AdultSelfHarm", "selfharm", "SuicideWatch",
    "Guilt", "Pessimism", "selfhelp", "whatsbotheringyou"
]

def dataset_generation():
  """Build control and symptom datasets"""
  # remove posts with deleted authors
  cleaned = df[df["author"] != '[deleted]']
  output_map = {}
  for symptom in subreddits:
    output_map[symptom] = cleaned[cleaned['subreddit'].isin(subreddits[symptom])]["text"]

  all_depressed = cleaned[cleaned['subreddit'].isin(depression_subreddits)]
  print(all_depressed.head())
  # This groups all depressed posts by author and takes the minimum post timestamp
  users = all_depressed[["author", "created_utc"]].groupby(["author"]).min()
  print(users.head())

  not_depressed = cleaned[~cleaned['subreddit'].isin(depression_subreddits)]
  # This join will combine our minimum depressed post timestamp with our data for posts in non-depressed subreddits
  joined = not_depressed.join(users, on="author", how="inner", rsuffix="_depressed", validate="m:1")
  print(joined)

  # We can then filter by posts that are before this date
  control = joined[joined["created_utc"] < joined["created_utc_depressed"] - (180*86400)]["text"]
  return (output_map, control)

depressed, control = dataset_generation()

for d in depressed:
  print(d, depressed[d].shape)
print(control.shape)

from happiestfuntokenizing.happiestfuntokenizing import Tokenizer

tokenizer = Tokenizer()
def tokenize():
  """Tokenize"""
  tokenized = {}
  # apply tokenizer to each document
  for s in depressed:
    tokenized[s] = depressed[s].apply(tokenizer.tokenize)
    print(s)
  return tokenized, control.apply(tokenizer.tokenize)

depressed_tok, control_tok = tokenize()

from gensim.corpora import Dictionary

# Create a gensim dictionary with all control documents and add
# all the documents in the depressed subreddits
dct = Dictionary(control_tok)
for s in depressed_tok:
  dct.add_documents(depressed_tok[s])

# filter out the 100 most frequent stop words
dct.filter_n_most_frequent(100)

# Apply the gensim document to bag of words model to each document
depressed_bow = {}
for s in depressed:
  depressed_bow[s] = depressed_tok[s].apply(dct.doc2bow)
  print(s)
control_bow = control_tok.apply(dct.doc2bow)

"""## Reddit Topics with LDA

 - Don't use MALLET (as the paper does), use some other LDA implementation.
"""

# TODO: Your LDA code!
from gensim.models import LdaMulticore
# Create a corpus with all documents
corpus = pd.concat([control_bow, pd.concat([depressed_bow[s] for s in depressed_bow])])
# Train our LDA on the corpus
lda = LdaMulticore(corpus, num_topics=200, id2word=dct)

import torch
torch.cuda.empty_cache()

"""## RoBERTa Embeddings"""

# TODO: Your RoBERTa code!
from transformers import RobertaModel, RobertaTokenizerFast

# Initialize our roberta model and tokenizer
model = RobertaModel.from_pretrained("roberta-base")
rob_tokenizer = RobertaTokenizerFast.from_pretrained("roberta-base")
device = torch.device("cuda")
model.to(device)
model.eval()

from os.path import exists
import math
batch_size = 64

def get_roberta_embeddings(dataset, name):
  filename = f"{FILEPATH}roberta_embeddings_{name}.csv"
  # cache embeddings (total generation takes about 40 minutes)
  if exists(filename):
    out = np.loadtxt(filename)
    return out
  out = []
  with torch.no_grad():
    # batch our documents
    for i in range(math.ceil(len(dataset) / batch_size)):
      batch = dataset[batch_size*i:batch_size*(i+1)]
      # tokenize our batch
      dataset_rob_tok = rob_tokenizer(batch.to_list(), return_tensors='pt', padding=True, truncation=True, max_length=512)
      dataset_rob_tok.to(device)
      # apply the roberta model to our tokens
      embedding = model(**dataset_rob_tok, output_hidden_states=True)
      # take the average of our 10th hidden state among all words as per the paper
      avg_10th_layer = torch.mean(embedding.hidden_states[9], 1).cpu().numpy()
      if i % 10 == 0:
        print(i, " batches out of ", len(dataset) // batch_size)
      out.extend(avg_10th_layer)
    np.savetxt(filename, out)
  return out

roberta_control = get_roberta_embeddings(control, "Control")

roberta_depressed = {}
for s in depressed:
  print(s)
  roberta_depressed[s] = get_roberta_embeddings(depressed[s], s)

def get_lda_topics(docs, name):
  filename = f"{FILEPATH}lda_topics_{name}.csv"
  # cache lda topics
  if exists(filename):
    out = np.loadtxt(filename)
    return out
  control_lda_topics = np.zeros([len(docs), 200])
  topics = [lda.get_document_topics(doc, minimum_probability=0.01) for doc in docs]
  # get_document_topics returns a list of (index, value) pairs,
  # so we need to convert into a topic-document matrix
  for doc_ind in range(len(topics)):
    for ind, data in topics[doc_ind]:
      control_lda_topics[doc_ind, ind] = data
  np.savetxt(filename, control_lda_topics)
  return control_lda_topics

lda_control = get_lda_topics(control_bow, "Control")

lda_depressed = {}
for s in depressed:
  print(s)
  lda_depressed[s] = get_lda_topics(depressed_bow[s], s)

"""## Main"""

def main(X, y):
  """
  Here's the basic structure of the main block! It should run
  5-fold cross validation with random forest to evaluate your RoBERTa and LDA
  performance.
  """
  rf_classifier = RandomForestClassifier()
  cv = KFold(n_splits=5, shuffle=True)
  results = cross_validate(rf_classifier, X=X, y=y, cv=cv, scoring='roc_auc', return_train_score=True)

  # TODO: Print your training and testing scores!
  print("test: ", np.average(results["test_score"]), " train: ",  np.average(results["train_score"]))
  pass

print("LDA")
for s in depressed:
  print(s)
  data = np.concatenate((lda_control, lda_depressed[s]))
  labels = [0 if i < len(lda_control) else 1 for i in range(len(data))]
  main(data, labels)

print("Roberta")
for s in roberta_depressed:
  print(s)
  data = np.concatenate((roberta_control, roberta_depressed[s]))
  labels = [0 if i < len(roberta_control) else 1 for i in range(len(data))]
  main(data, labels)