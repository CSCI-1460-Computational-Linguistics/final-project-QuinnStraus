{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reddit Depression Final Project\n",
        "Link to the paper: https://dl.acm.org/doi/pdf/10.1145/3578503.3583621\n",
        "\n",
        "Read through the paper fully before starting the assignment!"
      ],
      "metadata": {
        "id": "9jFvbbC6VtZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install happiestfuntokenizing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R253r2YRFnfr",
        "outputId": "cce613a4-da94-446b-d46b-644f0b985e11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting happiestfuntokenizing\n",
            "  Using cached happiestfuntokenizing-0.0.7-py3-none-any.whl\n",
            "Installing collected packages: happiestfuntokenizing\n",
            "Successfully installed happiestfuntokenizing-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FoBxKQ_OVl-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afe0c92-f458-4213-900f-f34cad02543d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_validate, cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "FILEPATH = 'drive/My Drive/'\n",
        "drive.mount('/content/drive',  force_remount=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "rcMOTL7mV9T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load():\n",
        "  \"\"\"Load pickles\"\"\"\n",
        "  with open(f\"{FILEPATH}Copy of student.pkl\", 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "    f.close()\n",
        "  return df\n",
        "\n",
        "df = load()"
      ],
      "metadata": {
        "id": "icnzto8GWFlb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A map from symptom name to included subreddits\n",
        "subreddits = {\n",
        "    \"Anger\": [\"Anger\"],\n",
        "    \"Anhedonia\": [\"anhedonia\", \"DeadBedrooms\",],\n",
        "    \"Anxiety\": [\"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\"],\n",
        "    \"Disordered Eating\": [\"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\"],\n",
        "    \"Loneliness\": [\"ForeverAlone\", \"lonely\"],\n",
        "    \"Sad mood\": [\"cry\", \"grief\", \"sad\", \"Sadness\"],\n",
        "    \"Self-loathing\": [\"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\"],\n",
        "    \"Sleep problem\": [\"insomnia\", \"sleep\"],\n",
        "    \"Somatic complaint\": [\"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\"],\n",
        "    \"Worthlessness\": [\"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"]\n",
        "}\n",
        "\n",
        "# List of depression subreddits in the paper\n",
        "depression_subreddits = [\"Anger\",\n",
        "    \"anhedonia\", \"DeadBedrooms\",\n",
        "    \"Anxiety\", \"AnxietyDepression\", \"HealthAnxiety\", \"PanicAttack\",\n",
        "    \"DecisionMaking\", \"shouldi\",\n",
        "    \"bingeeating\", \"BingeEatingDisorder\", \"EatingDisorders\", \"eating_disorders\", \"EDAnonymous\",\n",
        "    \"chronicfatigue\", \"Fatigue\",\n",
        "    \"ForeverAlone\", \"lonely\",\n",
        "    \"cry\", \"grief\", \"sad\", \"Sadness\",\n",
        "    \"AvPD\", \"SelfHate\", \"selfhelp\", \"socialanxiety\", \"whatsbotheringyou\",\n",
        "    \"insomnia\", \"sleep\",\n",
        "    \"cfs\", \"ChronicPain\", \"Constipation\", \"EssentialTremor\", \"headaches\", \"ibs\", \"tinnitus\",\n",
        "    \"AdultSelfHarm\", \"selfharm\", \"SuicideWatch\",\n",
        "    \"Guilt\", \"Pessimism\", \"selfhelp\", \"whatsbotheringyou\"\n",
        "]\n",
        "\n",
        "def dataset_generation():\n",
        "  \"\"\"Build control and symptom datasets\"\"\"\n",
        "  # remove posts with deleted authors\n",
        "  cleaned = df[df[\"author\"] != '[deleted]']\n",
        "  output_map = {}\n",
        "  for symptom in subreddits:\n",
        "    output_map[symptom] = cleaned[cleaned['subreddit'].isin(subreddits[symptom])][\"text\"]\n",
        "\n",
        "  all_depressed = cleaned[cleaned['subreddit'].isin(depression_subreddits)]\n",
        "  print(all_depressed.head())\n",
        "  # This groups all depressed posts by author and takes the minimum post timestamp\n",
        "  users = all_depressed[[\"author\", \"created_utc\"]].groupby([\"author\"]).min()\n",
        "  print(users.head())\n",
        "\n",
        "  not_depressed = cleaned[~cleaned['subreddit'].isin(depression_subreddits)]\n",
        "  # This join will combine our minimum depressed post timestamp with our data for posts in non-depressed subreddits\n",
        "  joined = not_depressed.join(users, on=\"author\", how=\"inner\", rsuffix=\"_depressed\", validate=\"m:1\")\n",
        "  print(joined)\n",
        "\n",
        "  # We can then filter by posts that are before this date\n",
        "  control = joined[joined[\"created_utc\"] < joined[\"created_utc_depressed\"] - (180*86400)][\"text\"]\n",
        "  return (output_map, control)\n",
        "\n",
        "depressed, control = dataset_generation()"
      ],
      "metadata": {
        "id": "Wpw9kJiras4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93da7a93-9ebb-4171-e8a5-8105dc801d05"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text           author  \\\n",
            "20  i'm trying hi, i'm sorry if my writing is bad,...        n90300118   \n",
            "39  Only friend has been blanking me for what feel...  Throwaway34qwas   \n",
            "67  Study hall social anxiety bruh We had a study ...         Shwin280   \n",
            "72  Positive Thoughts For You - We Are Happy To Pu...       pthinkimag   \n",
            "79  Starting from a blowup mattress Today was a ve...      MyCrazyLove   \n",
            "\n",
            "       subreddit  created_utc     date  \n",
            "20  SuicideWatch   1510374743  ression  \n",
            "39        lonely   1505308711  ression  \n",
            "67       Anxiety   1515634258  ression  \n",
            "72       Anxiety   1515944819  ression  \n",
            "79  SuicideWatch   1516594948  ression  \n",
            "                  created_utc\n",
            "author                       \n",
            "---annon---        1505320579\n",
            "---michelle---     1505417481\n",
            "--Solus            1500350545\n",
            "--broken_wings--   1500326111\n",
            "--closer2thesun    1508050449\n",
            "                                                      text  \\\n",
            "0        does your life feel like a waste mines not a c...   \n",
            "3661     can you still buy pro duo memory cards in stor...   \n",
            "11635                          unsure what to do [removed]   \n",
            "17094    Do you think nature changing will be a thing i...   \n",
            "25541    Do you think pokemon switch will be a third pe...   \n",
            "...                                                    ...   \n",
            "1967447  [QUESTION] ANYBODY KNOW OF PICKING EXERCISES? ...   \n",
            "1967679  The shop I work at was broken into last night....   \n",
            "1968747  sick of my miserable existence, donâ€™t know wha...   \n",
            "1968850  Afraid I'm a real pedo... At first I thought i...   \n",
            "1969014                           Cyroaudiovascularmexia.    \n",
            "\n",
            "                       author     subreddit  created_utc     date  \\\n",
            "0                  trademeple    depression   1504920055  2017-09   \n",
            "3661               trademeple           PSP   1504086485  2017-08   \n",
            "11635              trademeple  GetMotivated   1504089285  2017-08   \n",
            "17094              trademeple       pokemon   1511422241  2017-11   \n",
            "25541              trademeple       pokemon   1508748079  2017-10   \n",
            "...                       ...           ...          ...      ...   \n",
            "1967447           danklord555        Guitar   1500343305  2017-07   \n",
            "1967679         BenTheDude100      Sneakers   1510853163  2017-11   \n",
            "1968747  Throwawayaccount6245    offmychest   1513392148  2017-12   \n",
            "1968850    darkvalley13376262    depression   1506476797  2017-09   \n",
            "1969014             FrostyJib        Tinder   1514436478  2017-12   \n",
            "\n",
            "         created_utc_depressed  \n",
            "0                   1501502957  \n",
            "3661                1501502957  \n",
            "11635               1501502957  \n",
            "17094               1501502957  \n",
            "25541               1501502957  \n",
            "...                        ...  \n",
            "1967447             1503391863  \n",
            "1967679             1515857510  \n",
            "1968747             1513394020  \n",
            "1968850             1506477601  \n",
            "1969014             1514679884  \n",
            "\n",
            "[1149172 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for d in depressed:\n",
        "  print(d, depressed[d].shape)\n",
        "print(control.shape)"
      ],
      "metadata": {
        "id": "ohOK3wCdWpnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3fbc69-5985-4cd8-840e-e6b5d09bef9c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anger (552,)\n",
            "Anhedonia (5911,)\n",
            "Anxiety (24428,)\n",
            "Disordered Eating (1789,)\n",
            "Loneliness (11485,)\n",
            "Sad mood (2215,)\n",
            "Self-loathing (9831,)\n",
            "Sleep problem (3174,)\n",
            "Somatic complaint (8322,)\n",
            "Worthlessness (1804,)\n",
            "(4369,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from happiestfuntokenizing.happiestfuntokenizing import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "def tokenize():\n",
        "  \"\"\"Tokenize\"\"\"\n",
        "  tokenized = {}\n",
        "  # apply tokenizer to each document\n",
        "  for s in depressed:\n",
        "    tokenized[s] = depressed[s].apply(tokenizer.tokenize)\n",
        "    print(s)\n",
        "  return tokenized, control.apply(tokenizer.tokenize)\n",
        "\n",
        "depressed_tok, control_tok = tokenize()"
      ],
      "metadata": {
        "id": "MWGVUju_WxuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deca5510-fc77-4b5d-aeb1-2f027c52b063"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anger\n",
            "Anhedonia\n",
            "Anxiety\n",
            "Disordered Eating\n",
            "Loneliness\n",
            "Sad mood\n",
            "Self-loathing\n",
            "Sleep problem\n",
            "Somatic complaint\n",
            "Worthlessness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Create a gensim dictionary with all control documents and add\n",
        "# all the documents in the depressed subreddits\n",
        "dct = Dictionary(control_tok)\n",
        "for s in depressed_tok:\n",
        "  dct.add_documents(depressed_tok[s])\n",
        "\n",
        "# filter out the 100 most frequent stop words\n",
        "dct.filter_n_most_frequent(100)"
      ],
      "metadata": {
        "id": "xJgkXVFwkYq6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the gensim document to bag of words model to each document\n",
        "depressed_bow = {}\n",
        "for s in depressed:\n",
        "  depressed_bow[s] = depressed_tok[s].apply(dct.doc2bow)\n",
        "  print(s)\n",
        "control_bow = control_tok.apply(dct.doc2bow)\n"
      ],
      "metadata": {
        "id": "Q3j9z7UuW3eG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299133cb-94d8-4465-b685-51909e3a08be"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anger\n",
            "Anhedonia\n",
            "Anxiety\n",
            "Disordered Eating\n",
            "Loneliness\n",
            "Sad mood\n",
            "Self-loathing\n",
            "Sleep problem\n",
            "Somatic complaint\n",
            "Worthlessness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reddit Topics with LDA\n",
        "\n",
        " - Don't use MALLET (as the paper does), use some other LDA implementation."
      ],
      "metadata": {
        "id": "U4I37U1SXAEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Your LDA code!\n",
        "from gensim.models import LdaMulticore\n",
        "# Create a corpus with all documents\n",
        "corpus = pd.concat([control_bow, pd.concat([depressed_bow[s] for s in depressed_bow])])\n",
        "# Train our LDA on the corpus\n",
        "lda = LdaMulticore(corpus, num_topics=200, id2word=dct)"
      ],
      "metadata": {
        "id": "xf3surfWXH-q"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "sJRLgtKzhGIF"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RoBERTa Embeddings"
      ],
      "metadata": {
        "id": "E0-97hsVXNkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Your RoBERTa code!\n",
        "from transformers import RobertaModel, RobertaTokenizerFast\n",
        "\n",
        "# Initialize our roberta model and tokenizer\n",
        "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "rob_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "blx1SWVMXYDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af9c6c5-d9cb-45b1-b020-9a06af19e95b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): RobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x RobertaLayer(\n",
              "        (attention): RobertaAttention(\n",
              "          (self): RobertaSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): RobertaSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): RobertaIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): RobertaOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): RobertaPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import exists\n",
        "import math\n",
        "batch_size = 64\n",
        "\n",
        "def get_roberta_embeddings(dataset, name):\n",
        "  filename = f\"{FILEPATH}roberta_embeddings_{name}.csv\"\n",
        "  # cache embeddings (total generation takes about 40 minutes)\n",
        "  if exists(filename):\n",
        "    out = np.loadtxt(filename)\n",
        "    return out\n",
        "  out = []\n",
        "  with torch.no_grad():\n",
        "    # batch our documents\n",
        "    for i in range(math.ceil(len(dataset) / batch_size)):\n",
        "      batch = dataset[batch_size*i:batch_size*(i+1)]\n",
        "      # tokenize our batch\n",
        "      dataset_rob_tok = rob_tokenizer(batch.to_list(), return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "      dataset_rob_tok.to(device)\n",
        "      # apply the roberta model to our tokens\n",
        "      embedding = model(**dataset_rob_tok, output_hidden_states=True)\n",
        "      # take the average of our 10th hidden state among all words as per the paper\n",
        "      avg_10th_layer = torch.mean(embedding.hidden_states[9], 1).cpu().numpy()\n",
        "      if i % 10 == 0:\n",
        "        print(i, \" batches out of \", len(dataset) // batch_size)\n",
        "      out.extend(avg_10th_layer)\n",
        "    np.savetxt(filename, out)\n",
        "  return out"
      ],
      "metadata": {
        "id": "ZtV-NoXNd190"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_control = get_roberta_embeddings(control, \"Control\")"
      ],
      "metadata": {
        "id": "Orby17qRlT2J"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_depressed = {}\n",
        "for s in depressed:\n",
        "  print(s)\n",
        "  roberta_depressed[s] = get_roberta_embeddings(depressed[s], s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBmt4zcymtGm",
        "outputId": "8c5a8825-7600-4633-fcb1-7903fc5668f9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anger\n",
            "Anhedonia\n",
            "Anxiety\n",
            "Disordered Eating\n",
            "Loneliness\n",
            "Sad mood\n",
            "Self-loathing\n",
            "Sleep problem\n",
            "Somatic complaint\n",
            "Worthlessness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lda_topics(docs, name):\n",
        "  filename = f\"{FILEPATH}lda_topics_{name}.csv\"\n",
        "  # cache lda topics\n",
        "  if exists(filename):\n",
        "    out = np.loadtxt(filename)\n",
        "    return out\n",
        "  control_lda_topics = np.zeros([len(docs), 200])\n",
        "  topics = [lda.get_document_topics(doc, minimum_probability=0.01) for doc in docs]\n",
        "  # get_document_topics returns a list of (index, value) pairs,\n",
        "  # so we need to convert into a topic-document matrix\n",
        "  for doc_ind in range(len(topics)):\n",
        "    for ind, data in topics[doc_ind]:\n",
        "      control_lda_topics[doc_ind, ind] = data\n",
        "  np.savetxt(filename, control_lda_topics)\n",
        "  return control_lda_topics"
      ],
      "metadata": {
        "id": "kugD-YYyIIa4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_control = get_lda_topics(control_bow, \"Control\")"
      ],
      "metadata": {
        "id": "3n-fdVAMRGiL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_depressed = {}\n",
        "for s in depressed:\n",
        "  print(s)\n",
        "  lda_depressed[s] = get_lda_topics(depressed_bow[s], s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hew7v3sdwUKK",
        "outputId": "3729ba74-f5c3-46c3-e794-7b7f98e1c504"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Anger\n",
            "Anhedonia\n",
            "Anxiety\n",
            "Disordered Eating\n",
            "Loneliness\n",
            "Sad mood\n",
            "Self-loathing\n",
            "Sleep problem\n",
            "Somatic complaint\n",
            "Worthlessness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "rDWxuF2jXtwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(X, y):\n",
        "  \"\"\"\n",
        "  Here's the basic structure of the main block! It should run\n",
        "  5-fold cross validation with random forest to evaluate your RoBERTa and LDA\n",
        "  performance.\n",
        "  \"\"\"\n",
        "  rf_classifier = RandomForestClassifier()\n",
        "  cv = KFold(n_splits=5, shuffle=True)\n",
        "  results = cross_validate(rf_classifier, X=X, y=y, cv=cv, scoring='roc_auc', return_train_score=True)\n",
        "\n",
        "  # TODO: Print your training and testing scores!\n",
        "  print(\"test: \", np.average(results[\"test_score\"]), \" train: \",  np.average(results[\"train_score\"]))\n",
        "  pass\n",
        "\n",
        "print(\"LDA\")\n",
        "for s in depressed:\n",
        "  print(s)\n",
        "  data = np.concatenate((lda_control, lda_depressed[s]))\n",
        "  labels = [0 if i < len(lda_control) else 1 for i in range(len(data))]\n",
        "  main(data, labels)\n",
        "\n",
        "print(\"Roberta\")\n",
        "for s in roberta_depressed:\n",
        "  print(s)\n",
        "  data = np.concatenate((roberta_control, roberta_depressed[s]))\n",
        "  labels = [0 if i < len(roberta_control) else 1 for i in range(len(data))]\n",
        "  main(data, labels)\n"
      ],
      "metadata": {
        "id": "koTBPhcDXujb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "571db4bf-f44b-4039-daac-da4810299f6e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA\n",
            "Anger\n",
            "test:  0.9191300690107067  train:  0.9998989838594664\n",
            "Anhedonia\n",
            "test:  0.9558534749898884  train:  0.9994775185704938\n",
            "Anxiety\n",
            "test:  0.9073781050631629  train:  0.9993114521603328\n",
            "Disordered Eating\n",
            "test:  0.9510997453716102  train:  0.9990392950382831\n",
            "Loneliness\n",
            "test:  0.8592465088976875  train:  0.9996150062374669\n",
            "Sad mood\n",
            "test:  0.8606586881379963  train:  0.9986228766661593\n",
            "Self-loathing\n",
            "test:  0.8643708048882084  train:  0.9988977570094761\n",
            "Sleep problem\n",
            "test:  0.977634153624553  train:  0.9995266904378066\n",
            "Somatic complaint\n",
            "test:  0.9116715946316155  train:  0.9990556186613964\n",
            "Worthlessness\n",
            "test:  0.7645791837921697  train:  0.9973185372336524\n",
            "Roberta\n",
            "Anger\n",
            "test:  0.9055782908088947  train:  1.0\n",
            "Anhedonia\n",
            "test:  0.9400506805251119  train:  1.0\n",
            "Anxiety\n",
            "test:  0.93563502223511  train:  1.0\n",
            "Disordered Eating\n",
            "test:  0.9242114433202809  train:  1.0\n",
            "Loneliness\n",
            "test:  0.8893823862304593  train:  0.9999623761421944\n",
            "Sad mood\n",
            "test:  0.915475828980411  train:  1.0\n",
            "Self-loathing\n",
            "test:  0.9074727786365513  train:  0.999984415365908\n",
            "Sleep problem\n",
            "test:  0.9283422525655449  train:  1.0\n",
            "Somatic complaint\n",
            "test:  0.9047752651639179  train:  1.0\n",
            "Worthlessness\n",
            "test:  0.8886350529994929  train:  1.0\n"
          ]
        }
      ]
    }
  ]
}